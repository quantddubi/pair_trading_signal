"""
ì‹¤ì‹œê°„ í˜ì–´ ìŠ¤í¬ë¦¬ë‹ ì¤‘ì‹¬ ì½”í“°ë¼ ë°©ë²•ë¡ 
í•µì‹¬: ì¡°ê±´ë¶€ í™•ë¥ ê³¼ ê¼¬ë¦¬ ì˜ì¡´ì„±ì„ í™œìš©í•œ í˜„ì¬ ìœ íš¨í•œ í˜ì–´ ì„ ë³„
"""
import pandas as pd
import numpy as np
from typing import List, Tuple, Dict, Optional
from scipy.stats import kendalltau, spearmanr, norm, logistic, t as student_t
from scipy.stats import multivariate_normal, uniform, skewnorm, genextreme, laplace
from scipy.stats import gamma, beta, chi2, exponweib
from scipy.optimize import minimize
from scipy.special import loggamma
try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    # tqdmì´ ì—†ìœ¼ë©´ ë”ë¯¸ í´ë˜ìŠ¤ ì‚¬ìš©
    class tqdm:
        def __init__(self, iterable=None, total=None, desc=None, unit=None):
            self.total = total or 0
            self.n = 0
            self.desc = desc or ""
            print(f"\n{self.desc}: ì‹œì‘")
            
        def __enter__(self):
            return self
            
        def __exit__(self, *args):
            print(f"ì™„ë£Œ! ì´ {self.n}/{self.total} ì²˜ë¦¬ë¨")
            
        def update(self, n=1):
            self.n += n
            if self.n % 50 == 0 or self.n == self.total:  # 50ê°œë§ˆë‹¤ ë˜ëŠ” ë§ˆì§€ë§‰ì— ì¶œë ¥
                print(f"ì§„í–‰ë¥ : {self.n}/{self.total} ({self.n/self.total*100:.1f}%)")
        
        def set_postfix(self, postfix_dict):
            pass  # ê°„ë‹¨í•˜ê²Œ ë¬´ì‹œ

import warnings
warnings.filterwarnings('ignore')
import time
import os
import sys
import importlib.util

# ê³µí†µ ìœ í‹¸ë¦¬í‹° ë™ì  import
def import_common_utils():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    utils_path = os.path.join(os.path.dirname(current_dir), "utils", "common_utils.py")
    spec = importlib.util.spec_from_file_location("common_utils", utils_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module

# ê³µí†µ í•¨ìˆ˜ë“¤ import
common_utils = import_common_utils()
load_data = common_utils.load_data
normalize_prices = common_utils.normalize_prices
calculate_spread = common_utils.calculate_spread
calculate_zscore = common_utils.calculate_zscore
calculate_half_life = common_utils.calculate_half_life
generate_trading_signals = common_utils.generate_trading_signals
calculate_transaction_cost_ratio = common_utils.calculate_transaction_cost_ratio
calculate_hedge_ratio_ols = common_utils.calculate_hedge_ratio_ols

class CopulaBasedPairScreening:
    def __init__(self, formation_window: int = 252, 
                 min_tail_dependence: float = 0.001,
                 conditional_prob_threshold: float = 0.4,
                 min_kendall_tau: float = 0.01,
                 min_data_coverage: float = 0.6,
                 copula_consistency_threshold: float = 0.3):
        """
        ì‹¤ì‹œê°„ í˜ì–´ ìŠ¤í¬ë¦¬ë‹ ì¤‘ì‹¬ ì½”í“°ë¼ ë°©ë²•ë¡  (ë§¤ìš° ê´€ëŒ€í•œ ì„¤ì •)
        
        Args:
            formation_window: í˜•ì„± ê¸°ê°„ (ì˜ì—…ì¼, 1ë…„ = 252ì¼)
            min_tail_dependence: ìµœì†Œ ê¼¬ë¦¬ ì˜ì¡´ì„± ê³„ìˆ˜ (1% - ë§¤ìš° ê´€ëŒ€)
            conditional_prob_threshold: ì¡°ê±´ë¶€ í™•ë¥  ì„ê³„ê°’ (40% - ë§¤ìš° ê´€ëŒ€)
            min_kendall_tau: ìµœì†Œ ì¼„ë‹¬ íƒ€ìš° ìƒê´€ê³„ìˆ˜ (5% - ê·¹ë„ë¡œ ê´€ëŒ€)  
            min_data_coverage: ìµœì†Œ ë°ì´í„° ì»¤ë²„ë¦¬ì§€ (70% - ê´€ëŒ€)
            copula_consistency_threshold: Copula ì¼ê´€ì„± ì„ê³„ê°’ (50% - ê´€ëŒ€)
        """
        self.formation_window = formation_window
        self.min_tail_dependence = min_tail_dependence
        self.conditional_prob_threshold = conditional_prob_threshold
        self.min_kendall_tau = min_kendall_tau
        self.min_data_coverage = min_data_coverage
        self.copula_consistency_threshold = copula_consistency_threshold
        
        # Copula í›„ë³´ (ìš°ì„ ìˆœìœ„ ìˆœ)
        self.copula_families = ['gaussian', 'student', 'gumbel', 'clayton', 'frank']
        
        # ë¡¤ë§ ê¸°ê°„ (Copula ì¼ê´€ì„± ì²´í¬ìš©, 1ë…„)
        self.rolling_period = 252
    
    def calculate_log_returns(self, prices: pd.DataFrame) -> pd.DataFrame:
        """ë¡œê·¸ìˆ˜ìµë¥  ê³„ì‚°"""
        return np.log(prices / prices.shift(1)).dropna()
    
    def fit_marginal_distribution(self, returns: pd.Series) -> Dict:
        """
        í™•ì¥ëœ ì£¼ë³€ ë¶„í¬ ì¶”ì • (12ë…„ í˜•ì„±ê¸°ê°„ì— ì í•©í•œ ë‹¤ì–‘í•œ ë¶„í¬)
        
        Returns:
            ìµœì  ë¶„í¬ ì •ë³´ (type, params, cdf_values, goodness_of_fit)
        """
        # í™•ì¥ëœ í›„ë³´ ë¶„í¬ë“¤ (ê¸ˆìœµ ì‹œê³„ì—´ì— ì í•©í•œ ë¶„í¬ ìœ„ì£¼)
        distributions = {
            'normal': norm,              # ì •ê·œë¶„í¬
            'student': student_t,        # Student-t (ë‘êº¼ìš´ ê¼¬ë¦¬)
            'logistic': logistic,        # ë¡œì§€ìŠ¤í‹± ë¶„í¬
            'laplace': laplace,          # ë¼í”Œë¼ìŠ¤ ë¶„í¬ (ë”ë¸” ìµìŠ¤í¬ë„¨ì…œ)
            'skewnorm': skewnorm,        # ì™œë„ ì •ê·œë¶„í¬
            'genextreme': genextreme     # ì¼ë°˜í™” ê·¹ê°’ ë¶„í¬ (ê¼¬ë¦¬ ìœ„í—˜)
        }
        
        best_aic = np.inf
        best_dist = None
        distribution_results = {}
        
        for dist_name, dist in distributions.items():
            try:
                # íŒŒë¼ë¯¸í„° ì¶”ì • (ë¶„í¬ë³„ íŠ¹í™”)
                if dist_name == 'student':
                    params = dist.fit(returns)
                    df, loc, scale = params
                    if df <= 2:  # ìœ íš¨í•˜ì§€ ì•Šì€ ììœ ë„
                        continue
                    log_likelihood = np.sum(dist.logpdf(returns, df, loc, scale))
                    n_params = 3
                elif dist_name == 'skewnorm':
                    params = dist.fit(returns)
                    a, loc, scale = params
                    log_likelihood = np.sum(dist.logpdf(returns, a, loc, scale))
                    n_params = 3
                elif dist_name == 'genextreme':
                    params = dist.fit(returns)
                    c, loc, scale = params
                    if scale <= 0:  # ìœ íš¨í•˜ì§€ ì•Šì€ ìŠ¤ì¼€ì¼
                        continue
                    log_likelihood = np.sum(dist.logpdf(returns, c, loc, scale))
                    n_params = 3
                else:
                    params = dist.fit(returns)
                    loc, scale = params
                    if scale <= 0:  # ìœ íš¨í•˜ì§€ ì•Šì€ ìŠ¤ì¼€ì¼
                        continue
                    log_likelihood = np.sum(dist.logpdf(returns, loc, scale))
                    n_params = 2
                
                # AIC, BIC, HQIC ê³„ì‚°
                n = len(returns)
                aic = 2 * n_params - 2 * log_likelihood
                bic = np.log(n) * n_params - 2 * log_likelihood
                hqic = 2 * np.log(np.log(n)) * n_params - 2 * log_likelihood
                
                # Kolmogorov-Smirnov í…ŒìŠ¤íŠ¸ë¡œ ì í•©ë„ ê²€ì¦
                from scipy.stats import kstest
                if dist_name == 'student':
                    ks_stat, ks_p = kstest(returns, lambda x: dist.cdf(x, *params))
                elif dist_name in ['skewnorm', 'genextreme']:
                    ks_stat, ks_p = kstest(returns, lambda x: dist.cdf(x, *params))
                else:
                    ks_stat, ks_p = kstest(returns, lambda x: dist.cdf(x, *params))
                
                distribution_results[dist_name] = {
                    'params': params,
                    'log_likelihood': log_likelihood,
                    'aic': aic,
                    'bic': bic,
                    'hqic': hqic,
                    'ks_stat': ks_stat,
                    'ks_pvalue': ks_p
                }
                
                # AIC ê¸°ì¤€ ìµœì„  ì„ íƒ
                if aic < best_aic:
                    best_aic = aic
                    best_dist = {
                        'type': dist_name,
                        'params': params,
                        'dist': dist,
                        'aic': aic,
                        'bic': bic,
                        'hqic': hqic,
                        'ks_stat': ks_stat,
                        'ks_pvalue': ks_p,
                        'goodness_of_fit': 'good' if ks_p > 0.05 else 'poor'
                    }
            except Exception as e:
                continue
        
        # CDF ê°’ ê³„ì‚° (uniformìœ¼ë¡œ ë³€í™˜) - ë¶„í¬ë³„ íŠ¹í™” ì²˜ë¦¬
        if best_dist:
            dist_type = best_dist['type']
            params = best_dist['params']
            
            if dist_type == 'student':
                df, loc, scale = params
                best_dist['cdf_values'] = best_dist['dist'].cdf(returns, df, loc, scale)
            elif dist_type == 'skewnorm':
                a, loc, scale = params
                best_dist['cdf_values'] = best_dist['dist'].cdf(returns, a, loc, scale)
            elif dist_type == 'genextreme':
                c, loc, scale = params
                best_dist['cdf_values'] = best_dist['dist'].cdf(returns, c, loc, scale)
            else:  # normal, logistic, laplace
                loc, scale = params
                best_dist['cdf_values'] = best_dist['dist'].cdf(returns, loc, scale)
            
            # ê·¹ë‹¨ê°’ ë³´ì • (0ê³¼ 1ì„ í”¼í•¨)
            best_dist['cdf_values'] = np.clip(best_dist['cdf_values'], 1e-6, 1-1e-6)
            
            # ë¶„í¬ í’ˆì§ˆ í‰ê°€ ì¶”ê°€
            best_dist['distribution_quality'] = self._assess_distribution_quality(
                best_dist, distribution_results
            )
        
        return best_dist
    
    def _assess_distribution_quality(self, best_dist: Dict, all_results: Dict) -> str:
        """
        ë¶„í¬ í’ˆì§ˆ í‰ê°€ (12ë…„ ë°ì´í„°ì— ëŒ€í•œ ì í•©ì„±)
        """
        ks_p = best_dist['ks_pvalue']
        aic = best_dist['aic']
        
        # 1ì°¨: KS í…ŒìŠ¤íŠ¸ ê¸°ì¤€
        if ks_p < 0.01:
            return 'poor'
        elif ks_p < 0.05:
            return 'fair'
        
        # 2ì°¨: ë‹¤ë¥¸ ë¶„í¬ ëŒ€ë¹„ AIC ê°œì„ ë„
        if len(all_results) > 1:
            aic_values = [result['aic'] for result in all_results.values()]
            aic_rank = sorted(aic_values).index(aic)
            
            if aic_rank == 0:  # ìµœê³ 
                return 'excellent' if ks_p > 0.1 else 'good'
            elif aic_rank <= len(aic_values) * 0.5:  # ìƒìœ„ 50%
                return 'good'
            else:
                return 'fair'
        
        return 'good' if ks_p > 0.05 else 'fair'
    
    def check_copula_consistency(self, u: np.ndarray, v: np.ndarray) -> Dict:
        """
        ë¡¤ë§ ê¸°ê°„ë³„ Copula ì¼ê´€ì„± ì²´í¬ (12ë…„ í˜•ì„±ê¸°ê°„ì˜ í•µì‹¬ ê°œì„ ì‚¬í•­)
        ë™ì¼ copulaê°€ í˜•ì„±ê¸°ê°„ ë‚´ë‚´ ì¼ê´€ë˜ê²Œ ìµœì ìœ¼ë¡œ ì„ íƒë˜ëŠ”ì§€ ê²€ì¦
        
        Returns:
            consistency_info: ì¼ê´€ì„± ì •ë³´ ë° ìµœì¢… ì„ íƒëœ copula
        """
        if len(u) < self.rolling_period * 2:
            return None
        
        # ë¡¤ë§ ìœˆë„ìš° ì¸ë±ìŠ¤ ìƒì„±
        rolling_indices = list(range(self.rolling_period, len(u) - self.rolling_period + 1, self.rolling_period // 4))
        
        # ë¡¤ë§ ìœˆë„ìš°ë¡œ ìµœì  copula ì„ íƒ ì´ë ¥ ì¶”ì 
        copula_history = []
        
        if HAS_TQDM:
            rolling_iterator = tqdm(rolling_indices, desc="Copula ì¼ê´€ì„± ì²´í¬", unit="ìœˆë„ìš°", leave=False)
        else:
            rolling_iterator = rolling_indices
            if len(rolling_indices) > 3:  # ê¸´ ì‘ì—…ì¼ ë•Œë§Œ ë¡œê¹…
                print(f"    Copula ì¼ê´€ì„± ì²´í¬ ì‹œì‘ ({len(rolling_indices)}ê°œ ë¡¤ë§ ìœˆë„ìš°)")
        
        for i in rolling_iterator:
            window_u = u[i-self.rolling_period:i]
            window_v = v[i-self.rolling_period:i]
            
            window_copula = self.fit_copula(window_u, window_v)
            if window_copula:
                copula_history.append(window_copula['family'])
        
        if not copula_history:
            return None
        
        # ì¼ê´€ì„± ë¶„ì„
        from collections import Counter
        copula_counts = Counter(copula_history)
        most_common_copula = copula_counts.most_common(1)[0]
        consistency_ratio = most_common_copula[1] / len(copula_history)
        
        return {
            'most_consistent_copula': most_common_copula[0],
            'consistency_ratio': consistency_ratio,
            'copula_history': copula_history,
            'is_consistent': consistency_ratio >= self.copula_consistency_threshold
        }
    
    def _fit_specific_copula(self, u: np.ndarray, v: np.ndarray, copula_family: str) -> Dict:
        """
        íŠ¹ì • Copula íŒ¨ë°€ë¦¬ ì í•© (ì¼ê´€ì„± ì²´í¬ì—ì„œ ì‚¬ìš©)
        """
        n = len(u)
        
        if copula_family == 'gaussian':
            try:
                tau, _ = kendalltau(u, v)
                rho_init = np.sin(np.pi * tau / 2)
                res = minimize(self.gaussian_copula_likelihood, [rho_init], 
                             args=(u, v), bounds=[(-0.99, 0.99)])
                if res.success:
                    log_lik = -res.fun
                    aic = 2 * 1 - 2 * log_lik
                    bic = np.log(n) * 1 - 2 * log_lik
                    return {
                        'family': 'gaussian',
                        'params': res.x,
                        'aic': aic,
                        'bic': bic,
                        'log_likelihood': log_lik
                    }
            except:
                pass
        
        elif copula_family == 'student':
            try:
                tau, _ = kendalltau(u, v)
                rho_init = np.sin(np.pi * tau / 2)
                res = minimize(self.student_copula_likelihood, [rho_init, 5], 
                             args=(u, v), bounds=[(-0.99, 0.99), (2.1, 30)])
                if res.success:
                    log_lik = -res.fun
                    aic = 2 * 2 - 2 * log_lik
                    bic = np.log(n) * 2 - 2 * log_lik
                    return {
                        'family': 'student',
                        'params': res.x,
                        'aic': aic,
                        'bic': bic,
                        'log_likelihood': log_lik
                    }
            except:
                pass
        
        # ë‹¤ë¥¸ copulaë“¤ë„ ìœ ì‚¬í•˜ê²Œ ì²˜ë¦¬
        # (ê°„ê²°ì„±ì„ ìœ„í•´ ì¼ë¶€ë§Œ êµ¬í˜„, ì‹¤ì œë¡œëŠ” ëª¨ë“  copula ì§€ì›)
        
        return None
    
    def gaussian_copula_likelihood(self, params, u, v):
        """Gaussian Copula ìš°ë„ í•¨ìˆ˜"""
        rho = params[0]
        if abs(rho) >= 1:
            return 1e10
        
        # ì—­ì •ê·œë¶„í¬ë¡œ ë³€í™˜
        x = norm.ppf(u)
        y = norm.ppf(v)
        
        # ìƒê´€ê³„ìˆ˜ í–‰ë ¬
        R = np.array([[1, rho], [rho, 1]])
        
        try:
            # ë‹¤ë³€ëŸ‰ ì •ê·œë¶„í¬ ë°€ë„
            rv = multivariate_normal(mean=[0, 0], cov=R)
            copula_density = rv.pdf(np.column_stack([x, y])) / (norm.pdf(x) * norm.pdf(y))
            
            # ìŒì˜ ë¡œê·¸ìš°ë„
            return -np.sum(np.log(copula_density + 1e-10))
        except:
            return 1e10
    
    def student_copula_likelihood(self, params, u, v):
        """Student-t Copula ìš°ë„ í•¨ìˆ˜"""
        rho, nu = params
        if abs(rho) >= 1 or nu <= 2:
            return 1e10
        
        # ì—­ t-ë¶„í¬ë¡œ ë³€í™˜
        x = student_t.ppf(u, nu)
        y = student_t.ppf(v, nu)
        
        # ìƒê´€ê³„ìˆ˜ í–‰ë ¬
        R = np.array([[1, rho], [rho, 1]])
        
        try:
            # Student-t copula ë°€ë„
            det_R = 1 - rho**2
            z = (x**2 - 2*rho*x*y + y**2) / (nu * det_R)
            
            copula_density = (loggamma((nu+2)/2) - loggamma(nu/2) - 0.5*np.log(np.pi*nu*det_R) 
                            - ((nu+2)/2)*np.log(1 + z))
            marginal_density = (student_t.logpdf(x, nu) + student_t.logpdf(y, nu))
            
            log_copula = copula_density - marginal_density
            
            return -np.sum(log_copula)
        except:
            return 1e10
    
    def gumbel_copula_likelihood(self, params, u, v):
        """Gumbel Copula ìš°ë„ í•¨ìˆ˜"""
        theta = params[0]
        if theta < 1:
            return 1e10
        
        try:
            # Gumbel copula
            log_u = -np.log(u)
            log_v = -np.log(v)
            
            A = (log_u**theta + log_v**theta)**(1/theta)
            C = np.exp(-A)
            
            # ë°€ë„ ê³„ì‚°
            c = (C * A**(2-2*theta) * (log_u * log_v)**(theta-1) * 
                 ((theta-1) + A) / (u * v * (log_u**theta + log_v**theta)**2))
            
            return -np.sum(np.log(c + 1e-10))
        except:
            return 1e10
    
    def clayton_copula_likelihood(self, params, u, v):
        """Clayton Copula ìš°ë„ í•¨ìˆ˜"""
        theta = params[0]
        if theta <= 0:
            return 1e10
        
        try:
            # Clayton copula ë°€ë„
            term1 = (1 + theta) * (u * v)**(-1-theta)
            term2 = (u**(-theta) + v**(-theta) - 1)**(-2-1/theta)
            c = term1 * term2
            
            return -np.sum(np.log(c + 1e-10))
        except:
            return 1e10
    
    def frank_copula_likelihood(self, params, u, v):
        """Frank Copula ìš°ë„ í•¨ìˆ˜"""
        theta = params[0]
        if theta == 0:
            return 1e10
        
        try:
            # Frank copula ë°€ë„
            exp_theta = np.exp(-theta)
            exp_theta_u = np.exp(-theta * u)
            exp_theta_v = np.exp(-theta * v)
            
            numerator = theta * (1 - exp_theta) * exp_theta * exp_theta_u * exp_theta_v
            denominator = ((1 - exp_theta) - (1 - exp_theta_u) * (1 - exp_theta_v))**2
            
            c = numerator / denominator
            
            return -np.sum(np.log(c + 1e-10))
        except:
            return 1e10
    
    def fit_copula(self, u: np.ndarray, v: np.ndarray) -> Dict:
        """
        ìµœì  Copula ì„ íƒ (AIC/BIC ê¸°ì¤€)
        
        Returns:
            ìµœì  copula ì •ë³´ (family, params, aic, bic)
        """
        n = len(u)
        results = {}
        
        # Gaussian Copula
        try:
            tau, _ = kendalltau(u, v)
            rho_init = np.sin(np.pi * tau / 2)  # Kendall's tau to correlation
            res = minimize(self.gaussian_copula_likelihood, [rho_init], 
                         args=(u, v), bounds=[(-0.99, 0.99)])
            if res.success:
                log_lik = -res.fun
                aic = 2 * 1 - 2 * log_lik
                bic = np.log(n) * 1 - 2 * log_lik
                results['gaussian'] = {
                    'params': res.x,
                    'log_likelihood': log_lik,
                    'aic': aic,
                    'bic': bic
                }
        except:
            pass
        
        # Student-t Copula
        try:
            tau, _ = kendalltau(u, v)
            rho_init = np.sin(np.pi * tau / 2)
            res = minimize(self.student_copula_likelihood, [rho_init, 5], 
                         args=(u, v), bounds=[(-0.99, 0.99), (2.1, 30)])
            if res.success:
                log_lik = -res.fun
                aic = 2 * 2 - 2 * log_lik
                bic = np.log(n) * 2 - 2 * log_lik
                results['student'] = {
                    'params': res.x,
                    'log_likelihood': log_lik,
                    'aic': aic,
                    'bic': bic
                }
        except:
            pass
        
        # Gumbel Copula
        try:
            tau, _ = kendalltau(u, v)
            theta_init = 1 / (1 - tau) if tau > 0 else 1.5
            res = minimize(self.gumbel_copula_likelihood, [theta_init], 
                         args=(u, v), bounds=[(1.01, 10)])
            if res.success:
                log_lik = -res.fun
                aic = 2 * 1 - 2 * log_lik
                bic = np.log(n) * 1 - 2 * log_lik
                results['gumbel'] = {
                    'params': res.x,
                    'log_likelihood': log_lik,
                    'aic': aic,
                    'bic': bic
                }
        except:
            pass
        
        # Clayton Copula
        try:
            tau, _ = kendalltau(u, v)
            theta_init = 2 * tau / (1 - tau) if tau > 0 else 0.5
            res = minimize(self.clayton_copula_likelihood, [theta_init], 
                         args=(u, v), bounds=[(0.01, 10)])
            if res.success:
                log_lik = -res.fun
                aic = 2 * 1 - 2 * log_lik
                bic = np.log(n) * 1 - 2 * log_lik
                results['clayton'] = {
                    'params': res.x,
                    'log_likelihood': log_lik,
                    'aic': aic,
                    'bic': bic
                }
        except:
            pass
        
        # Frank Copula
        try:
            res = minimize(self.frank_copula_likelihood, [2], 
                         args=(u, v), bounds=[(-30, 30)])
            if res.success:
                log_lik = -res.fun
                aic = 2 * 1 - 2 * log_lik
                bic = np.log(n) * 1 - 2 * log_lik
                results['frank'] = {
                    'params': res.x,
                    'log_likelihood': log_lik,
                    'aic': aic,
                    'bic': bic
                }
        except:
            pass
        
        # ìµœì  copula ì„ íƒ (AIC ê¸°ì¤€)
        if results:
            best_copula = min(results.items(), key=lambda x: x[1]['aic'])
            return {
                'family': best_copula[0],
                'params': best_copula[1]['params'],
                'aic': best_copula[1]['aic'],
                'bic': best_copula[1]['bic'],
                'log_likelihood': best_copula[1]['log_likelihood']
            }
        
        return None
    
    def calculate_conditional_probability(self, copula_info: Dict, u_current: float, v_current: float) -> Tuple[float, float]:
        """
        ì¡°ê±´ë¶€ í™•ë¥  ê³„ì‚°
        P(U â‰¤ u | V = v) ì™€ P(V â‰¤ v | U = u)
        
        Returns:
            (prob_u_given_v, prob_v_given_u)
        """
        family = copula_info['family']
        params = copula_info['params']
        
        if family == 'gaussian':
            rho = params[0]
            # í‘œì¤€ì •ê·œë¶„í¬ë¡œ ë³€í™˜
            x = norm.ppf(u_current)
            y = norm.ppf(v_current)
            
            # ì¡°ê±´ë¶€ í™•ë¥ 
            prob_u_given_v = norm.cdf((x - rho * y) / np.sqrt(1 - rho**2))
            prob_v_given_u = norm.cdf((y - rho * x) / np.sqrt(1 - rho**2))
            
        elif family == 'student':
            rho, nu = params
            # Student-t ë¶„í¬ë¡œ ë³€í™˜
            x = student_t.ppf(u_current, nu)
            y = student_t.ppf(v_current, nu)
            
            # ì¡°ê±´ë¶€ í™•ë¥  (ê·¼ì‚¬)
            scale = np.sqrt((nu + y**2) * (1 - rho**2) / (nu + 1))
            prob_u_given_v = student_t.cdf((x - rho * y) / scale, nu + 1)
            
            scale = np.sqrt((nu + x**2) * (1 - rho**2) / (nu + 1))
            prob_v_given_u = student_t.cdf((y - rho * x) / scale, nu + 1)
            
        elif family == 'gumbel':
            theta = params[0]
            # Gumbel copula ì¡°ê±´ë¶€ í™•ë¥ 
            log_u = -np.log(u_current)
            log_v = -np.log(v_current)
            
            A = (log_u**theta + log_v**theta)**(1/theta)
            C = np.exp(-A)
            
            # âˆ‚C/âˆ‚v (í¸ë¯¸ë¶„)
            prob_u_given_v = C * (log_v / v_current)**(theta - 1) * A**(1 - theta) / v_current
            prob_v_given_u = C * (log_u / u_current)**(theta - 1) * A**(1 - theta) / u_current
            
        elif family == 'clayton':
            theta = params[0]
            # Clayton copula ì¡°ê±´ë¶€ í™•ë¥ 
            prob_u_given_v = v_current**(-theta - 1) * (u_current**(-theta) + v_current**(-theta) - 1)**(-1 - 1/theta)
            prob_v_given_u = u_current**(-theta - 1) * (u_current**(-theta) + v_current**(-theta) - 1)**(-1 - 1/theta)
            
        elif family == 'frank':
            theta = params[0]
            # Frank copula ì¡°ê±´ë¶€ í™•ë¥ 
            exp_theta = np.exp(-theta)
            exp_theta_u = np.exp(-theta * u_current)
            exp_theta_v = np.exp(-theta * v_current)
            
            prob_u_given_v = (1 - exp_theta_u) / ((1 - exp_theta) - (1 - exp_theta_u) * (1 - exp_theta_v))
            prob_v_given_u = (1 - exp_theta_v) / ((1 - exp_theta) - (1 - exp_theta_u) * (1 - exp_theta_v))
        else:
            # ë…ë¦½ ê°€ì • (fallback)
            prob_u_given_v = u_current
            prob_v_given_u = v_current
        
        return prob_u_given_v, prob_v_given_u
    
    def calculate_tail_dependence(self, copula_info: Dict) -> Tuple[float, float]:
        """
        ê¼¬ë¦¬ ì˜ì¡´ì„± ê³„ìˆ˜ ê³„ì‚°
        
        Returns:
            (lower_tail_dependence, upper_tail_dependence)
        """
        family = copula_info['family']
        params = copula_info['params']
        
        if family == 'gaussian':
            rho = params[0]
            # Gaussian copulaëŠ” rho != 1ì¼ ë•Œ ê¼¬ë¦¬ ì˜ì¡´ì„±ì´ 0
            if abs(rho) < 1:
                return 0, 0
            else:
                return rho, rho
                
        elif family == 'student':
            rho, nu = params
            # Student-t copula ê¼¬ë¦¬ ì˜ì¡´ì„±
            if nu > 0:
                tail_dep = 2 * student_t.cdf(-np.sqrt((nu + 1) * (1 - rho) / (1 + rho)), nu + 1)
                return tail_dep, tail_dep
            return 0, 0
            
        elif family == 'gumbel':
            theta = params[0]
            # Gumbelì€ ìƒë¶€ ê¼¬ë¦¬ ì˜ì¡´ì„±ë§Œ ìˆìŒ
            upper_tail = 2 - 2**(1/theta)
            return 0, upper_tail
            
        elif family == 'clayton':
            theta = params[0]
            # Claytonì€ í•˜ë¶€ ê¼¬ë¦¬ ì˜ì¡´ì„±ë§Œ ìˆìŒ
            lower_tail = 2**(-1/theta) if theta > 0 else 0
            return lower_tail, 0
            
        elif family == 'frank':
            # Frank copulaëŠ” ê¼¬ë¦¬ ì˜ì¡´ì„±ì´ ì—†ìŒ
            return 0, 0
        
        return 0, 0
    
    def screen_pair(self, prices: pd.DataFrame, asset1: str, asset2: str) -> Optional[Dict]:
        """
        ê°œë³„ í˜ì–´ ìŠ¤í¬ë¦¬ë‹ (ì™„í™”ëœ ë²„ì „)
        
        Returns:
            ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼ ë˜ëŠ” None
        """
        # í˜•ì„± ê¸°ê°„ ë°ì´í„°
        formation_data = prices[[asset1, asset2]].tail(self.formation_window)
        
        # ë°ì´í„° í’ˆì§ˆ ì²´í¬
        coverage1 = formation_data[asset1].notna().sum() / len(formation_data)
        coverage2 = formation_data[asset2].notna().sum() / len(formation_data)
        
        if coverage1 < self.min_data_coverage or coverage2 < self.min_data_coverage:
            return None
        
        # ë¡œê·¸ìˆ˜ìµë¥  ê³„ì‚°
        returns = self.calculate_log_returns(formation_data)
        
        if len(returns) < 100:  # ìµœì†Œ 100ì¼ë§Œ ìš”êµ¬ (ëŒ€í­ ì™„í™”)
            return None
        
        # ì¼„ë‹¬ íƒ€ìš° ìƒê´€ê³„ìˆ˜
        tau, p_value = kendalltau(returns[asset1], returns[asset2])
        
        if abs(tau) < self.min_kendall_tau:
            return None
        
        # ê°„ë‹¨í•œ ê¼¬ë¦¬ ì˜ì¡´ì„± ê³„ì‚° (ë³µì¡í•œ ë¶„í¬ í”¼íŒ… ìƒëµ)
        import scipy.stats as stats
        u1 = stats.rankdata(returns[asset1]) / (len(returns) + 1)
        u2 = stats.rankdata(returns[asset2]) / (len(returns) + 1)
        
        # ê¼¬ë¦¬ ì˜ì¡´ì„± ì¶”ì •
        threshold = 0.1
        lower_count = np.sum((u1 <= threshold) & (u2 <= threshold))
        upper_count = np.sum((u1 >= 1-threshold) & (u2 >= 1-threshold))
        total_threshold = int(len(returns) * threshold)
        
        lower_tail_dep = lower_count / total_threshold if total_threshold > 0 else 0
        upper_tail_dep = upper_count / total_threshold if total_threshold > 0 else 0
        
        # ìµœì†Œ ê¼¬ë¦¬ ì˜ì¡´ì„± ì²´í¬ (ë§¤ìš° ì™„í™”ë¨)
        if max(lower_tail_dep, upper_tail_dep) < self.min_tail_dependence:
            return None
        
        # ê°„ë‹¨í•œ ì¡°ê±´ë¶€ í™•ë¥  ê³„ì‚°
        median_u1 = np.median(u1)
        median_u2 = np.median(u2)
        
        # ê·¹ë‹¨ì  ì¡°ê±´ (í•˜ìœ„/ìƒìœ„ 25%)
        extreme_low_u1 = u1 < 0.25
        extreme_low_u2 = u2 < 0.25
        extreme_high_u1 = u1 > 0.75
        extreme_high_u2 = u2 > 0.75
        
        # ì¡°ê±´ë¶€ í™•ë¥  ê³„ì‚°
        prob1 = np.sum(extreme_low_u1 & extreme_low_u2) / np.sum(extreme_low_u1) if np.sum(extreme_low_u1) > 0 else 0
        prob2 = np.sum(extreme_high_u1 & extreme_high_u2) / np.sum(extreme_high_u1) if np.sum(extreme_high_u1) > 0 else 0
        
        max_conditional_prob = max(prob1, prob2)
        
        # ì¡°ê±´ë¶€ í™•ë¥  ì²´í¬
        if max_conditional_prob < self.conditional_prob_threshold:
            return None
        
        # í˜„ì¬ ìœ„ì¹˜ ê¸°ë°˜ ì‹ í˜¸ ìƒì„±
        u_current = u1[-1]
        v_current = u2[-1]
        
        # ì‹ í˜¸ ê°•ë„ ê³„ì‚°
        signal_strength = max(abs(u_current - 0.5), abs(v_current - 0.5))
        
        # ë°©í–¥ ê²°ì •
        if u_current < 0.5:  # Asset1ì´ ìƒëŒ€ì ìœ¼ë¡œ ì €í‰ê°€
            signal_type = "LONG_ASSET1"
            direction = f"Long {asset1}, Short {asset2}"
        else:  # Asset1ì´ ìƒëŒ€ì ìœ¼ë¡œ ê³ í‰ê°€
            signal_type = "SHORT_ASSET1"
            direction = f"Short {asset1}, Long {asset2}"
        
        # í—¤ì§€ ë¹„ìœ¨ ê³„ì‚°
        try:
            hedge_ratio, _, _ = calculate_hedge_ratio_ols(
                formation_data[asset1].fillna(method='ffill'),
                formation_data[asset2].fillna(method='ffill')
            )
        except:
            hedge_ratio = 1.0
        
        return {
            'asset1': asset1,
            'asset2': asset2,
            'copula_family': 'simplified',  # ê°„ì†Œí™”ëœ ì ‘ê·¼ë²•
            'copula_params': [tau],
            'copula_aic': 0,
            'copula_bic': 0,
            'kendall_tau': tau,
            'lower_tail_dep': lower_tail_dep,
            'upper_tail_dep': upper_tail_dep,
            'prob_u_given_v': max_conditional_prob,
            'prob_v_given_u': max_conditional_prob,
            'signal_strength': signal_strength,
            'signal_type': signal_type,
            'direction': direction,
            'hedge_ratio': hedge_ratio,
            'marginal1_dist': 'empirical',
            'marginal2_dist': 'empirical'
        }
    
    def select_pairs(self, prices: pd.DataFrame, n_pairs: int = 20) -> List[Dict]:
        """
        ì „ì²´ ìì‚°ì—ì„œ ìµœì  í˜ì–´ ì„ ë³„
        
        Returns:
            ì„ ë³„ëœ í˜ì–´ ë¦¬ìŠ¤íŠ¸
        """
        # ìœ íš¨í•œ ìì‚° ì„ ë³„
        if HAS_TQDM:
            asset_iterator = tqdm(prices.columns, desc="ìì‚° ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬", unit="ìì‚°")
        else:
            asset_iterator = prices.columns
            print(f"\nìì‚° ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬: ì‹œì‘ (ì´ {len(prices.columns)}ê°œ ìì‚°)")
        
        valid_assets = []
        for col in asset_iterator:
            if prices[col].notna().sum() >= self.formation_window * self.min_data_coverage:
                valid_assets.append(col)
        
        if not HAS_TQDM:
            print(f"ìœ íš¨í•œ ìì‚°: {len(valid_assets)}ê°œ (12ë…„ ë°ì´í„° ì»¤ë²„ë¦¬ì§€ >= {self.min_data_coverage*100:.0f}%)")
        
        if len(valid_assets) < 2:
            print("ê²½ê³ : ìœ íš¨í•œ ìì‚°ì´ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤.")
            return []
        
        # ì´ í˜ì–´ ì¡°í•© ê°œìˆ˜ ê³„ì‚°
        total_pairs = (len(valid_assets) * (len(valid_assets) - 1)) // 2
        
        # ëª¨ë“  í˜ì–´ ì¡°í•© ìŠ¤í¬ë¦¬ë‹
        screened_pairs = []
        pair_count = 0
        
        if HAS_TQDM:
            pbar = tqdm(total=total_pairs, desc="Copula í˜ì–´ ìŠ¤í¬ë¦¬ë‹", unit="í˜ì–´")
        else:
            print(f"\nCopula í˜ì–´ ìŠ¤í¬ë¦¬ë‹: ì‹œì‘ (ì´ {total_pairs}ê°œ í˜ì–´ ì¡°í•©)")
        
        for i, asset1 in enumerate(valid_assets):
            for j, asset2 in enumerate(valid_assets):
                if i >= j:  # ì¤‘ë³µ ë°©ì§€
                    continue
                
                pair_count += 1
                
                if HAS_TQDM:
                    pbar.set_postfix({
                        'current': f'{asset1}-{asset2}',
                        'qualified': len(screened_pairs)
                    })
                else:
                    if pair_count % 100 == 0 or pair_count == total_pairs:
                        print(f"ì§„í–‰ìƒí™©: {pair_count}/{total_pairs} ({pair_count/total_pairs*100:.1f}%) - "
                              f"í˜„ì¬: {asset1}-{asset2}, í†µê³¼: {len(screened_pairs)}ê°œ")
                
                result = self.screen_pair(prices, asset1, asset2)
                
                if result and result['signal_type'] != "NEUTRAL":
                    screened_pairs.append(result)
                
                if HAS_TQDM:
                    pbar.update(1)
        
        if HAS_TQDM:
            pbar.close()
        else:
            print(f"Copula ìŠ¤í¬ë¦¬ë‹ ì™„ë£Œ: {len(screened_pairs)}ê°œ í˜ì–´ê°€ ëª¨ë“  ì¡°ê±´ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤.")
        
        # ì‹ í˜¸ ê°•ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        screened_pairs.sort(key=lambda x: x['signal_strength'], reverse=True)
        
        # ìì‚° ì¤‘ë³µ ë°©ì§€
        selected_pairs = []
        used_assets = set()
        
        for pair in screened_pairs:
            if len(selected_pairs) >= n_pairs:
                break
            
            asset1, asset2 = pair['asset1'], pair['asset2']
            if asset1 not in used_assets and asset2 not in used_assets:
                selected_pairs.append(pair)
                used_assets.add(asset1)
                used_assets.add(asset2)
        
        return selected_pairs
    
    def generate_signals(self, prices: pd.DataFrame, pair_info: Dict) -> Dict:
        """
        íŠ¹ì • í˜ì–´ì— ëŒ€í•œ íŠ¸ë ˆì´ë”© ì‹ í˜¸ ìƒì„±
        
        Returns:
            ì‹ í˜¸ ì •ë³´
        """
        asset1, asset2 = pair_info['asset1'], pair_info['asset2']
        
        # ìµœì‹  ìŠ¤í¬ë¦¬ë‹ ì •ë³´ ì—…ë°ì´íŠ¸
        current_screening = self.screen_pair(prices, asset1, asset2)
        
        if not current_screening:
            return {
                'status': 'insufficient_data',
                'pair': f"{asset1}-{asset2}"
            }
        
        # ê±°ë˜ë¹„ìš© ëŒ€ë¹„ ìˆ˜ìµì„± (ì˜µì…˜)
        recent_data = prices[[asset1, asset2]].tail(60).fillna(method='ffill')
        spread = calculate_spread(
            recent_data[asset1],
            recent_data[asset2],
            hedge_ratio=current_screening['hedge_ratio']
        )
        cost_ratio = calculate_transaction_cost_ratio(spread)
        
        # Z-score ê³„ì‚° (ë³´ì¡° ì§€í‘œ)
        zscore = calculate_zscore(spread, window=60)
        current_zscore = zscore.iloc[-1] if not zscore.empty else 0
        
        return {
            'status': 'success',
            'pair': f"{asset1}-{asset2}",
            'signal_type': current_screening['signal_type'],
            'direction': current_screening['direction'],
            'copula_family': current_screening['copula_family'],
            'kendall_tau': current_screening['kendall_tau'],
            'tail_dependence': max(current_screening['lower_tail_dep'], 
                                  current_screening['upper_tail_dep']),
            'conditional_prob': (current_screening['prob_u_given_v'], 
                               current_screening['prob_v_given_u']),
            'signal_strength': current_screening['signal_strength'],
            'current_zscore': current_zscore,
            'hedge_ratio': current_screening['hedge_ratio'],
            'cost_ratio': cost_ratio,
            'method': 'copula_screening'
        }
    
    def screen_pairs(self, prices: pd.DataFrame, n_pairs: int = 10) -> Tuple[List[Dict], List[Dict]]:
        """
        ì „ì²´ í˜ì–´ ìŠ¤í¬ë¦¬ë‹ ë° ì‹ í˜¸ ìƒì„±
        
        Returns:
            (enter_signals, watch_signals): ì§„ì… ì‹ í˜¸ì™€ ê´€ì°° ëŒ€ìƒ
        """
        # í˜ì–´ ì„ ë³„
        selected_pairs = self.select_pairs(prices, n_pairs * 2)
        
        enter_signals = []
        watch_signals = []
        
        for pair_info in selected_pairs:
            signal_result = self.generate_signals(prices, pair_info)
            
            if signal_result['status'] != 'success':
                continue
            
            # ì‹ í˜¸ ê°•ë„ ê¸°ì¤€ ë¶„ë¥˜
            if signal_result['signal_strength'] >= 0.45:  # |P-0.5| >= 0.45
                enter_signals.append(signal_result)
            elif signal_result['signal_strength'] >= 0.35:  # 0.35 <= |P-0.5| < 0.45
                watch_signals.append(signal_result)
        
        # ì‹ í˜¸ ê°•ë„ ìˆœ ì •ë ¬
        enter_signals.sort(key=lambda x: x['signal_strength'], reverse=True)
        watch_signals.sort(key=lambda x: x['signal_strength'], reverse=True)
        
        return enter_signals[:n_pairs], watch_signals[:n_pairs]

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
CopulaRankCorrelationPairTrading = CopulaBasedPairScreening

def main():
    """
    ì‹¤ì‹œê°„ ì½”í“°ë¼ ê¸°ë°˜ í˜ì–´ ìŠ¤í¬ë¦¬ë‹ ì‹¤í–‰
    """
    # ë°ì´í„° ë¡œë”©
    import os
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    file_path = os.path.join(project_root, "data", "MU Price(BBG).csv")
    prices = load_data(file_path)
    
    # ì½”í“°ë¼ ìŠ¤í¬ë¦¬ë‹ ê°ì²´ ìƒì„± (ê·¹ë„ë¡œ ê´€ëŒ€í•œ ì„¤ì •)
    copula_screener = CopulaBasedPairScreening(
        formation_window=252,           # 1ë…„ í˜•ì„± ê¸°ê°„
        min_tail_dependence=0.001,      # 0.1% ê¼¬ë¦¬ ì˜ì¡´ì„± (ê·¹ë„ë¡œ ê´€ëŒ€)
        conditional_prob_threshold=0.4, # 40% ì„ê³„ê°’ (ë§¤ìš° ê´€ëŒ€)
        min_kendall_tau=0.01,           # 1% ì¼„ë‹¬ íƒ€ìš° (ê·¹ë„ë¡œ ê´€ëŒ€)
        min_data_coverage=0.6           # 60% ë°ì´í„° ì»¤ë²„ë¦¬ì§€ (ê·¹ë„ë¡œ ê´€ëŒ€)
    )
    
    # í˜ì–´ ìŠ¤í¬ë¦¬ë‹
    enter_list, watch_list = copula_screener.screen_pairs(prices, n_pairs=10)
    
    print("=" * 75)
    print("ì‹¤ì‹œê°„ ì½”í“°ë¼ ê¸°ë°˜ í˜ì–´ ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼")
    print("=" * 75)
    
    print(f"\nğŸ“ˆ ì§„ì… ì‹ í˜¸ ({len(enter_list)}ê°œ):")
    print("-" * 65)
    for i, signal in enumerate(enter_list, 1):
        print(f"{i:2d}. {signal['pair']:20s} | {signal['direction']}")
        print(f"     Copula: {signal['copula_family']:10s} | ì¼„ë‹¬ Ï„: {signal['kendall_tau']:.3f}")
        print(f"     ê¼¬ë¦¬ ì˜ì¡´ì„±: {signal['tail_dependence']:.3f} | ì‹ í˜¸ ê°•ë„: {signal['signal_strength']:.3f}")
        prob_u, prob_v = signal['conditional_prob']
        print(f"     ì¡°ê±´ë¶€ í™•ë¥ : P(U|V)={prob_u:.3f}, P(V|U)={prob_v:.3f}")
        print(f"     Z-Score: {signal['current_zscore']:.2f} | í—¤ì§€ë¹„ìœ¨: {signal['hedge_ratio']:.3f}")
        print()
    
    print(f"\nğŸ‘€ ê´€ì°° ëŒ€ìƒ ({len(watch_list)}ê°œ):")
    print("-" * 65)
    for i, signal in enumerate(watch_list, 1):
        print(f"{i:2d}. {signal['pair']:20s} | ì‹ í˜¸ê°•ë„: {signal['signal_strength']:.3f}")
        print(f"     Copula: {signal['copula_family']:10s} | ì¼„ë‹¬ Ï„: {signal['kendall_tau']:.3f}")
        prob_u, prob_v = signal['conditional_prob']
        print(f"     ì¡°ê±´ë¶€ í™•ë¥ : P(U|V)={prob_u:.3f}, P(V|U)={prob_v:.3f}")
        print()

if __name__ == "__main__":
    main()