#!/usr/bin/env python3
"""
ê° ë°©ë²•ë¡ ë³„ ë¶„ì„ ê²°ê³¼ë¥¼ ì‚¬ì „ ê³„ì‚°í•˜ì—¬ ìºì‹œ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
import os
import sys
import pickle
import json
from datetime import datetime
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

# ê° ë°©ë²•ë¡  ëª¨ë“ˆ import
import importlib.util

def import_module_from_file(file_path, module_name):
    spec = importlib.util.spec_from_file_location(module_name, file_path)
    module = importlib.util.module_from_spec(spec)
    sys.modules[module_name] = module
    spec.loader.exec_module(module)
    return module

# ê³µí†µ ìœ í‹¸ë¦¬í‹°
common_utils = import_module_from_file("utils/common_utils.py", "common_utils")

# ê° ë°©ë²•ë¡  ëª¨ë“ˆ
euclidean_module = import_module_from_file("methods/1_euclidean_distance_pairs.py", "euclidean_distance_pairs")
ssd_module = import_module_from_file("methods/2_ssd_distance_pairs.py", "ssd_distance_pairs")
cointegration_module = import_module_from_file("methods/3_cointegration_pairs.py", "cointegration_pairs")
regime_module = import_module_from_file("methods/4_correlation_regime_pairs.py", "correlation_regime_pairs")
ou_module = import_module_from_file("methods/5_ou_mean_reversion_pairs.py", "ou_mean_reversion_pairs")
clustering_module = import_module_from_file("methods/6_clustering_pairs.py", "clustering_pairs")
copula_module = import_module_from_file("methods/7_copula_rank_correlation_pairs.py", "copula_rank_correlation_pairs")

def get_data_last_date():
    """ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ë‚ ì§œ í™•ì¸"""
    file_path = "data/MU Price(BBG).csv"
    prices = common_utils.load_data(file_path)
    return prices.index[-1].strftime('%Y%m%d')

def generate_euclidean_cache():
    """ìœ í´ë¦¬ë“œ ê±°ë¦¬ ë°©ë²•ë¡  ìºì‹œ ìƒì„±"""
    print("ìœ í´ë¦¬ë“œ ê±°ë¦¬ ë°©ë²•ë¡  ìºì‹œ ìƒì„± ì¤‘...")
    
    file_path = "data/MU Price(BBG).csv"
    prices = common_utils.load_data(file_path)
    
    # ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¡œ ë¶„ì„
    trader = euclidean_module.EuclideanDistancePairTrading(
        formation_window=756,
        signal_window=756,
        enter_threshold=2.0,
        exit_threshold=0.5,
        stop_loss=3.0,
        min_half_life=5,
        max_half_life=60,
        min_cost_ratio=5.0,
        transaction_cost=0.0001
    )
    
    enter_list, watch_list = trader.screen_pairs(prices, n_pairs=10)
    
    cache_data = {
        'enter_signals': enter_list,
        'watch_signals': watch_list,
        'parameters': {
            'formation_window': 756,
            'signal_window': 756,
            'enter_threshold': 2.0,
            'exit_threshold': 0.5,
            'stop_loss': 3.0,
            'min_half_life': 5,
            'max_half_life': 60,
            'min_cost_ratio': 5.0,
            'transaction_cost': 0.0001
        },
        'generated_at': datetime.now().isoformat(),
        'data_date': get_data_last_date(),
        'method': 'euclidean'
    }
    
    # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    cache_dir = "cache"
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir)
    
    # ìºì‹œ ì €ì¥
    cache_file = os.path.join(cache_dir, "euclidean_default.pkl")
    with open(cache_file, 'wb') as f:
        pickle.dump(cache_data, f)
    
    print(f"âœ… ìœ í´ë¦¬ë“œ ê±°ë¦¬ ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(enter_list)}ê°œ ì§„ì…ì‹ í˜¸, {len(watch_list)}ê°œ ê´€ì°°ëŒ€ìƒ")
    return len(enter_list), len(watch_list)

def generate_ssd_cache():
    """SSD ê±°ë¦¬ ë°©ë²•ë¡  ìºì‹œ ìƒì„±"""
    print("ğŸ”„ SSD ê±°ë¦¬ ë°©ë²•ë¡  ìºì‹œ ìƒì„± ì¤‘...")
    
    file_path = "data/MU Price(BBG).csv"
    prices = common_utils.load_data(file_path)
    
    # ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¡œ ë¶„ì„
    trader = ssd_module.SSDDistancePairTrading(
        formation_window=252,
        signal_window=252,
        enter_threshold=2.0,
        exit_threshold=0.5,
        stop_loss=3.0,
        min_half_life=5,
        max_half_life=60,
        min_cost_ratio=5.0,
        transaction_cost=0.0001
    )
    
    enter_list, watch_list = trader.screen_pairs(prices, n_pairs=5)
    
    cache_data = {
        'enter_signals': enter_list,
        'watch_signals': watch_list,
        'parameters': {
            'formation_window': 252,
            'signal_window': 252,
            'enter_threshold': 2.0,
            'exit_threshold': 0.5,
            'stop_loss': 3.0,
            'min_half_life': 5,
            'max_half_life': 60,
            'min_cost_ratio': 5.0,
            'transaction_cost': 0.0001
        },
        'generated_at': datetime.now().isoformat(),
        'data_last_date': get_data_last_date()
    }
    
    # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    cache_dir = "cache"
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir)
    
    # ìºì‹œ ì €ì¥
    cache_file = os.path.join(cache_dir, "ssd_default.pkl")
    with open(cache_file, 'wb') as f:
        pickle.dump(cache_data, f)
    
    print(f"âœ… SSD ê±°ë¦¬ ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(enter_list)}ê°œ ì§„ì…ì‹ í˜¸, {len(watch_list)}ê°œ ê´€ì°°ëŒ€ìƒ")
    return len(enter_list), len(watch_list)

def generate_cointegration_cache():
    """ê³µì ë¶„ ë°©ë²•ë¡  ìºì‹œ ìƒì„±"""
    print("ğŸ”„ ê³µì ë¶„ ë°©ë²•ë¡  ìºì‹œ ìƒì„± ì¤‘...")
    
    file_path = "data/MU Price(BBG).csv"
    prices = common_utils.load_data(file_path)
    
    # ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¡œ ë¶„ì„
    trader = cointegration_module.CointegrationPairTrading(
        formation_window=252,
        signal_window=60,
        enter_threshold=2.0,
        exit_threshold=0.5,
        stop_loss=3.0,
        min_half_life=5,
        max_half_life=60,
        min_cost_ratio=5.0,
        max_pvalue=0.05
    )
    
    enter_list, watch_list = trader.screen_pairs(prices, n_pairs=5)
    
    cache_data = {
        'enter_signals': enter_list,
        'watch_signals': watch_list,
        'generated_at': datetime.now().isoformat(),
        'data_date': get_data_last_date(),
        'parameters': {
            'formation_window': 252,
            'signal_window': 60,
            'enter_threshold': 2.0,
            'exit_threshold': 0.5,
            'stop_loss': 3.0,
            'min_half_life': 5,
            'max_half_life': 60,
            'min_cost_ratio': 5.0,
            'max_pvalue': 0.05
        }
    }
    
    with open('cache/cointegration_default.pkl', 'wb') as f:
        pickle.dump(cache_data, f)
    
    print(f"âœ… ê³µì ë¶„ ìºì‹œ ì™„ë£Œ: {len(enter_list)}ê°œ ì§„ì… + {len(watch_list)}ê°œ ê´€ì°°")

def generate_regime_cache():
    """ìƒê´€ê´€ê³„ ë ˆì§ ë°©ë²•ë¡  ìºì‹œ ìƒì„±"""
    print("ğŸ”„ ìƒê´€ê´€ê³„ ë ˆì§ ë°©ë²•ë¡  ìºì‹œ ìƒì„± ì¤‘...")
    
    file_path = "data/MU Price(BBG).csv"
    prices = common_utils.load_data(file_path)
    
    trader = regime_module.CorrelationRegimePairTrading(
        formation_window=252,
        signal_window=60,
        long_corr_window=252,
        short_corr_window=60,
        enter_threshold=2.0,
        exit_threshold=0.5,
        stop_loss=3.0,
        min_half_life=5,
        max_half_life=60,
        min_cost_ratio=5.0,
        min_delta_corr=0.3
    )
    
    enter_list, watch_list = trader.screen_pairs(prices, n_pairs=5)
    
    cache_data = {
        'enter_signals': enter_list,
        'watch_signals': watch_list,
        'generated_at': datetime.now().isoformat(),
        'data_date': get_data_last_date(),
        'parameters': {
            'formation_window': 252,
            'signal_window': 60,
            'long_corr_window': 252,
            'short_corr_window': 60,
            'enter_threshold': 2.0,
            'exit_threshold': 0.5,
            'stop_loss': 3.0,
            'min_half_life': 5,
            'max_half_life': 60,
            'min_cost_ratio': 5.0,
            'min_delta_corr': 0.3
        }
    }
    
    with open('cache/regime_default.pkl', 'wb') as f:
        pickle.dump(cache_data, f)
    
    print(f"âœ… ë ˆì§ ìºì‹œ ì™„ë£Œ: {len(enter_list)}ê°œ ì§„ì… + {len(watch_list)}ê°œ ê´€ì°°")

def generate_ou_cache():
    """OU í‰ê· íšŒê·€ ë°©ë²•ë¡  ìºì‹œ ìƒì„±"""
    print("ğŸ”„ OU í‰ê· íšŒê·€ ë°©ë²•ë¡  ìºì‹œ ìƒì„± ì¤‘...")
    
    file_path = "data/MU Price(BBG).csv"
    prices = common_utils.load_data(file_path)
    
    trader = ou_module.OUMeanReversionPairTrading(
        formation_window=252,
        signal_window=60,
        enter_threshold=2.0,
        exit_threshold=0.5,
        stop_loss=3.0,
        min_half_life=5,
        max_half_life=60,
        min_cost_ratio=5.0,
        min_mean_reversion_speed=0.01
    )
    
    enter_list, watch_list = trader.screen_pairs(prices, n_pairs=5)
    
    cache_data = {
        'enter_signals': enter_list,
        'watch_signals': watch_list,
        'generated_at': datetime.now().isoformat(),
        'data_date': get_data_last_date(),
        'parameters': {
            'formation_window': 252,
            'signal_window': 60,
            'enter_threshold': 2.0,
            'exit_threshold': 0.5,
            'stop_loss': 3.0,
            'min_half_life': 5,
            'max_half_life': 60,
            'min_cost_ratio': 5.0,
            'min_mean_reversion_speed': 0.01
        }
    }
    
    with open('cache/ou_default.pkl', 'wb') as f:
        pickle.dump(cache_data, f)
    
    print(f"âœ… OU ìºì‹œ ì™„ë£Œ: {len(enter_list)}ê°œ ì§„ì… + {len(watch_list)}ê°œ ê´€ì°°")

def generate_clustering_cache():
    """í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•ë¡  ìºì‹œ ìƒì„±"""
    print("ğŸ”„ í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•ë¡  ìºì‹œ ìƒì„± ì¤‘...")
    
    file_path = "data/MU Price(BBG).csv"
    prices = common_utils.load_data(file_path)
    
    trader = clustering_module.ClusteringPairTrading(
        formation_window=252,
        signal_window=60,
        enter_threshold=2.0,
        exit_threshold=0.5,
        stop_loss=3.0,
        min_half_life=5,
        max_half_life=60,
        min_cost_ratio=5.0,
        n_clusters=8,
        clustering_method='kmeans'
    )
    
    enter_list, watch_list = trader.screen_pairs(prices, n_pairs=5)
    
    # í´ëŸ¬ìŠ¤í„° ìš”ì•½ ì •ë³´ë„ ì¶”ê°€
    cluster_summary = trader.get_cluster_summary(prices)
    
    cache_data = {
        'enter_signals': enter_list,
        'watch_signals': watch_list,
        'cluster_summary': cluster_summary,
        'generated_at': datetime.now().isoformat(),
        'data_date': get_data_last_date(),
        'parameters': {
            'formation_window': 252,
            'signal_window': 60,
            'enter_threshold': 2.0,
            'exit_threshold': 0.5,
            'stop_loss': 3.0,
            'min_half_life': 5,
            'max_half_life': 60,
            'min_cost_ratio': 5.0,
            'n_clusters': 8,
            'clustering_method': 'kmeans'
        }
    }
    
    with open('cache/clustering_default.pkl', 'wb') as f:
        pickle.dump(cache_data, f)
    
    print(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ìºì‹œ ì™„ë£Œ: {len(enter_list)}ê°œ ì§„ì… + {len(watch_list)}ê°œ ê´€ì°°")

def generate_copula_cache():
    """ì½”í“°ë¼ ìˆœìœ„ìƒê´€ ë°©ë²•ë¡  ìºì‹œ ìƒì„±"""
    print("ğŸ”„ ì½”í“°ë¼ ìˆœìœ„ìƒê´€ ë°©ë²•ë¡  ìºì‹œ ìƒì„± ì¤‘...")
    
    file_path = "data/MU Price(BBG).csv"
    prices = common_utils.load_data(file_path)
    
    trader = copula_module.CopulaRankCorrelationPairTrading(
        formation_window=252,
        signal_window=60,
        long_window=252,
        short_window=60,
        enter_threshold=2.0,
        exit_threshold=0.5,
        stop_loss=3.0,
        min_half_life=5,
        max_half_life=60,
        min_cost_ratio=5.0,
        min_rank_corr=0.3,
        min_rank_corr_change=0.2,
        tail_quantile=0.1
    )
    
    enter_list, watch_list = trader.screen_pairs(prices, n_pairs=5)
    
    cache_data = {
        'enter_signals': enter_list,
        'watch_signals': watch_list,
        'generated_at': datetime.now().isoformat(),
        'data_date': get_data_last_date(),
        'parameters': {
            'formation_window': 252,
            'signal_window': 60,
            'long_window': 252,
            'short_window': 60,
            'enter_threshold': 2.0,
            'exit_threshold': 0.5,
            'stop_loss': 3.0,
            'min_half_life': 5,
            'max_half_life': 60,
            'min_cost_ratio': 5.0,
            'min_rank_corr': 0.3,
            'min_rank_corr_change': 0.2,
            'tail_quantile': 0.1
        }
    }
    
    with open('cache/copula_default.pkl', 'wb') as f:
        pickle.dump(cache_data, f)
    
    print(f"âœ… ì½”í“°ë¼ ìºì‹œ ì™„ë£Œ: {len(enter_list)}ê°œ ì§„ì… + {len(watch_list)}ê°œ ê´€ì°°")

def main():
    """ëª¨ë“  ë°©ë²•ë¡  ìºì‹œ ìƒì„±"""
    print("=" * 60)
    print("ğŸš€ í˜ì–´íŠ¸ë ˆì´ë”© ë°©ë²•ë¡ ë³„ ìºì‹œ ìƒì„± ì‹œì‘")
    print("=" * 60)
    
    data_date = get_data_last_date()
    print(f"ğŸ“… ë°ì´í„° ê¸°ì¤€ì¼: {data_date}")
    
    try:
        # ê° ë°©ë²•ë¡ ë³„ ìºì‹œ ìƒì„± (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ìˆœì°¨ì ìœ¼ë¡œ)
        generate_euclidean_cache()
        generate_ssd_cache()
        generate_cointegration_cache()
        generate_regime_cache() 
        generate_ou_cache()
        generate_clustering_cache()
        generate_copula_cache()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ ëª¨ë“  ìºì‹œ ìƒì„± ì™„ë£Œ!")
        print("=" * 60)
        print(f"ìºì‹œ ìœ„ì¹˜: {os.path.abspath('cache')}")
        
    except Exception as e:
        print(f"âŒ ìºì‹œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main()